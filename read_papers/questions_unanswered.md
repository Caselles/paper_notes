I need the answer to these questions. Don't hesitate to contact me if you can help.

- Can we derive the discounted occupancy function of any policy using a forward model of a MDP ? 
- Why don't people use planning methods with successor representations ? Since you can estimate the reward function, you just need a forward model to plan. It seems like the whole point of SR, but seems useless then, since having a forward model is "easy".
- Could you define reward in terms of transitions ? Then with a forward model you can plan.
- How to define rewards as a function of states or high-level features of the env ? Apart from SR, which Q-learns the policy.
- In Direct Future Prediction (ICLR 17) they have measurements, that's why it works. Can we learn measurements or define reward in terms of a variable that is forecastable ? 
- Direct Future Prediction with goal as states ? 
- Which methods use a forward model but not a model of the reward ? Middle-ground between model-free and model-based.
- Which work use forward models of SMDPs ? A forward model for options.
- Lots of work that use TD-error methods to learn to solve mazes. Which approaches use a Monte-Carlo/Supervised Learning approach ? 
- If you have features of your states or MDP, how to define a goal using these ? Which works explore this ?
- Apart from planning, can you use model-base rollouts to find the best paths of transitions to maximize cumulative reward ? Like MCTS. 
