Inspired by [Adrian Colyer][1], [Denny Britz][2] and [Daniel Seita][3] 

This contains my notes for research papers that are relevant for my PhD on Machine Learning for Robotics. First, I list papers that I've read and papers that I want to read. Then, read papers are numbered on a (1) to (5) scale where a (1) means I have only barely skimmed it, while a (5) means I feel
confident that I understand almost everything about the paper. The links
here go to my paper summaries if I have them, otherwise those papers are on my
**TODO** list.

Contents:

- [Machine Learning for Robotics](#machine-learning-for-robotics)
    - [Papers I have read](#papers-i-have-read)
    - [Papers I want to read](#papers-i-want-to-read)

# Machine Learning for Robotics

## Papers I've read

- [Learning to Act by Predicting the Future](https://github.com/Caselles/paper_notes/blob/master/read_papers/learning_to_act_by_predicting_the_future.md) (5)
- [Universal Value Function Approximators](https://github.com/Caselles/paper_notes/blob/master/read_papers/universal_value_function_approximators.md) (4)
- [Horde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction](https://github.com/Caselles/paper_notes/blob/master/read_papers/horde_a_scalable_real_time.md) (5)
- [Learning by Playing â€“ Solving Sparse Reward Tasks from Scratch](https://github.com/Caselles/paper_notes/blob/master/read_papers/learning_by_playing_solving_sparse_reward_tasks_from_scratch.md) (3)
- [Reinforcement Learning with Unsupervised Auxiliary Tasks](https://github.com/Caselles/paper_notes/blob/master/read_papers/reinforcement_learning_with_unsupervised_auxiliary_tasks.md) (4)
- [Successor Features for Transfer in Reinforcement Learning
](https://github.com/Caselles/paper_notes/blob/master/read_papers/successor_features_for_transfer_in_reinforcemen_learning.md) (4)
- [Hindsight Experience Replay](https://github.com/Caselles/paper_notes/blob/master/read_papers/hindsight_experience_replay.md) (4)
- [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://github.com/Caselles/paper_notes/blob/master/read_papers/model_agnostic_meta_learning_for_fast_adaptation_of_deep_networks.md) (4)
- [Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm](https://github.com/Caselles/paper_notes/blob/master/read_papers/meta_learning_and_universality_deep_representations_and_gradient_descent_can_approximate_any_learning_algorithm.md) (2)
- [A Distributional Perspective on Reinforcement Learning](https://github.com/Caselles/paper_notes/blob/master/read_papers/a_distributional_perspective_on_reinforcement_learning.md) (4)
- [Transfer for Reinforcement Learning Domains: A Survey](https://github.com/Caselles/paper_notes/blob/master/read_papers/transfer_for_reinforcement_learning_domains_a_survey.md) (?)
- [World Models](https://github.com/Caselles/paper_notes/blob/master/read_papers/world_models.md) (4)
- [Simple random search provides a competitive approach to reinforcement learning](https://github.com/Caselles/paper_notes/blob/master/read_papers/simple_random_search_provides_a_competitive_approach%20to_reinforcement_learning.md) (5)


## Papers I want to read

- FeUdal Networks for Hierarchical Reinforcement Learning
- Diversity is All You Need: Learning Skills without a Reward Function
- Learning to Search Better than Your Teacher
- Transfer in Variable-Reward Hierarchical Reinforcement Learning
- Trust Region Policy Optimization
- Proximal Policy Optimization
- Curriculum Learning
- Theoretical TL papers from TL survey
- DRL that matters
- CMA-ES


[1]:https://blog.acolyer.org/about/
[2]:https://github.com/dennybritz/deeplearning-papernotes
[3]:https://github.com/DanielTakeshi/Paper_Notes#2018-rlil-papers
